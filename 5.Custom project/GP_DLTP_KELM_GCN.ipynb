{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imageio as io\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import normalize,StandardScaler\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import skimage.io as io\n",
    "import math\n",
    "from operator import eq\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from skimage import color\n",
    "from operator import eq\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FER:Face Emomtion Recognition model Using Dynamic Local Ternary Patterns With Kernel Extreme Learning Machine Classifier\n",
    "# In this model I use GCN to enhance input image as preprocessing step, DLTP feature descriptor, \n",
    "# PCA to reduce the high-dimensional DLTP features \n",
    "# and  K-ELM classifier to classify the face expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functional Description\n",
    "# •\tCropping:\n",
    "# This function crop input image with size 48x48\n",
    "# •\tConvertion:\n",
    "# Convert the cropped image into grayscale\n",
    "# •\tGlobal contrast normalization:\n",
    "# Enhance the grayscale image using GCN techniquies\n",
    "# •\tDynamic Local Ternary Pattern:\n",
    "# Get the ternary pattern of enhanced image using dynamic threshold value for every image and this ternary pattern is splited into two binary code P_DLTP & N_DLTP\n",
    "# •\tPositive dynamic Local Ternary Pattern:\n",
    "# Binary threshold function for positive pattern and get the weighted sum of it\n",
    "# •\tNegative dynamic Local Ternary Pattern:\n",
    "# Binary threshold function for negative pattern and get the weighted sum of it\n",
    "# •\tBuild the sub histogram :\n",
    "# the histograms are computed separately and the result is concatenated together\n",
    "# •\tBuild K-ELM  classifier:\n",
    "# The K-ELM classifier uses kernels that maps the features into\n",
    "# higher dimensional space and we use  the Gaussian kernels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pipeline consists of five units, namely preprocessing, image enhancement,feature extraction ,\n",
    "#dimensionality reduction and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "#random_seed parameter in split data function\n",
    "random_seed = 0  \n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "#control the width of gaussian kernel\n",
    "sigma=0.0008\n",
    "# regularization parameter\n",
    "c=64\n",
    "# regularization parameter\n",
    "reg=100\n",
    "# small constant error to avoid computation errors €\n",
    "err=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the quantized value of the surrounding neighbors of current pixel\n",
    "def DLTP(ic,arr1):\n",
    "    #the intensity value of the center pixel\n",
    "    Ic=ic\n",
    "    #8 neighbour of center pixel\n",
    "    arr=arr1[0]\n",
    "    #loop over 8 neighbour\n",
    "    for i in range(len(arr)-1):\n",
    "        #the intensity value of one neighbour pixel\n",
    "        In=arr[i]\n",
    "        #calculate the tao \"The threshold\" automatically\n",
    "        tao=abs(In-Ic)/(Ic+0.0001)\n",
    "        #Neighbor pixels that falls above Ic+tao to 1\n",
    "        if(In>Ic+tao):\n",
    "            arr[i]=1\n",
    "        #Neighbor pixels that falls below Ic-tao to -1    \n",
    "        elif(In<=Ic-tao):\n",
    "            arr[i]=-1\n",
    "        #Neighbor pixels that falls in between Ic+tao and Ic-tao are quantized to 0    \n",
    "        else:\n",
    "            arr[i]=0\n",
    "    return arr        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the postive DLTP pattern,by putting value of 1 in the postive places and otheres will be 0 \n",
    "def Pos_DLTP(arr):\n",
    "    pos_arr=np.zeros((1,8), np.uint8)[0]\n",
    "    k=0\n",
    "    for i in arr:\n",
    "        if (i==1):\n",
    "            pos_arr[k]=1\n",
    "        k=k+1    \n",
    "    return  pos_arr       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the negative DLTP pattern,by putting value of 1 in the negative places and otheres will be 0 \n",
    "def Neg_DLTP(arr):\n",
    "    neg_arr=np.zeros((1,8), np.uint8)[0]\n",
    "    k=0\n",
    "    for i in arr:\n",
    "        if (i==-1):\n",
    "            neg_arr[k]=1\n",
    "        k=k+1     \n",
    "    return  neg_arr       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting negative and positive binary patterns\n",
    "# are then multiplied with fixed weights and are summed up\n",
    "#to return DLTP encoded values\n",
    "def Convertion(arr):\n",
    "    value=0\n",
    "    arr_value=[1,2,4,8,16,32,64,128]\n",
    "    value=np.dot(arr,arr_value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad the current block with specific size\n",
    "def padding(block,size):\n",
    "    padd=np.pad(block, (size, size), 'constant', constant_values=(0, 0))\n",
    "    return padd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return 8 neighbour of current pixel\n",
    "def GetArray(block,x,y):\n",
    "    one=block[x-1][y+1]\n",
    "    two=block[x][y+1]\n",
    "    three=block[x+1][y+1]\n",
    "    four=block[x+1][y]\n",
    "    five=block[x-1][y+1]\n",
    "    six=block[x+1][y-1]\n",
    "    seven=block[x][y-1]\n",
    "    eight=block[x-1][y-1]\n",
    "    zero=block[x][y]\n",
    "    arr=[]\n",
    "    arr.append([one,two,three,four,five,six,seven,eight])\n",
    "    return zero,arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get both negative and positive DLTP histogram\n",
    "def DLTP_Hist(block):\n",
    "    img=padding(block,1)\n",
    "    w,l=img.shape\n",
    "    tlbphist=[]\n",
    "    neg_hist=[]\n",
    "    pos_hist=[]\n",
    "    #loop for every pixels in block\n",
    "    for y in range(1,w-1):\n",
    "        for x in range(1,l-1):\n",
    "            #return current pixel and 8 neighbour of current pixel\n",
    "            gc,array=GetArray(img,x,y)\n",
    "            #return DLTP code [1,0,-1]\n",
    "            array2=array1=DLTP(gc,array)\n",
    "            #the generated quantized value is further divided into negative and positive patterns\n",
    "            #get the negative and positive DLTP code\n",
    "            neg_arr=Neg_DLTP(array1)\n",
    "            pos_arr=Pos_DLTP(array2)\n",
    "            #calculate the weighted sum function for positive pattern and negative pattern to get PDLTP  and NDLTP\n",
    "            neg_hist.append(Convertion(neg_arr))\n",
    "            pos_hist.append(Convertion(pos_arr))\n",
    "    #calculate both pos and neg histogram separately        \n",
    "    (histpos, _) = np.histogram(np.array(pos_hist),bins=np.arange(0,255),range=(0,255))\n",
    "    histpos = histpos.astype(\"float\")\n",
    "    (histneg, _) = np.histogram(np.array(neg_hist),bins=np.arange(0,255),range=(0,255))\n",
    "    histneg = histneg.astype(\"float\")\n",
    "    return np.array(histneg).flatten(),np.array(histpos).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# •\tProperties of DLTP :\n",
    "# o\tIt is very useful in extracting facial texture information\n",
    "# o\tOvercomes the manual determination of threshold in the traditional LTP descriptor.\n",
    "# o\tThe threshold in DLTP is automatically determined using local neighborhood pixel intensities \n",
    "# o\tIt dynamically updates the threshold depending on the pixel intensity values\n",
    "#______________________________________________________________________________________________\n",
    "\n",
    "\n",
    "#Algorithm to extract the DLTP features\n",
    "def Algorithm_DLTP(img1,blocksize=4):\n",
    "    #get the width abd length of input image\n",
    "    w,l=img1.shape\n",
    "    #divided image into NxN blocks\n",
    "    window=w//blocksize\n",
    "    #two arrays to store both negative and positive DLTP histogram for all blocks\n",
    "    Ndltp_hist=[]\n",
    "    Pdltp_hist=[]\n",
    "    #pad image with 2 cells\n",
    "    img=padding(img1,2)\n",
    "    #loop over each block\n",
    "    for r in range(2,w-2,window):\n",
    "        for c in range(2,l-2,window):\n",
    "            #extract the block\n",
    "            block = img[r:r+window,c:c+window]\n",
    "            #get both positive and negative DLTP histogram\n",
    "            hneg,hpos= DLTP_Hist(block)\n",
    "            #append every positive and negative DLTP hist to two separate arrays\n",
    "            Pdltp_hist.append(np.array(hpos))\n",
    "            Ndltp_hist.append(np.array(hneg))\n",
    "    #array that store all positive and negative DLTP hist        \n",
    "    dltpfeature=[]        \n",
    "    #concatenate both histograms to get final features\n",
    "    dltpfeature.append(np.array(Pdltp_hist).flatten())\n",
    "    dltpfeature.append(np.array(Ndltp_hist).flatten())\n",
    "    return np.array(dltpfeature).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract DLTP features\n",
    "def feature_DLTP(imgs):\n",
    "    #process the image\n",
    "    gray=preprocessing1(imgs)\n",
    "    #enhance the image using GCN\n",
    "    enhanced=global_contrast_normalization(gray,reg,err)\n",
    "    #normalize the image\n",
    "    norm=min_max_normalize(enhanced)\n",
    "    return np.array(Algorithm_DLTP(norm,4))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the image\n",
    "def preprocessing1(img):\n",
    "    #resize the input image\n",
    "    resized = cv2.resize(img,(48,48), interpolation = cv2.INTER_AREA)\n",
    "    #convert image to gray image\n",
    "    gray=color.rgb2gray(resized)\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset:loop over all files of emotion\n",
    "def load_dataset():\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i, fn in enumerate(img_filenames):\n",
    "            #get the name of directory and store it as label of image\n",
    "            label = fn.split('.')[0]\n",
    "            #append the path of dataset and the new folder\n",
    "            path = os.path.join(path_to_dataset, fn)\n",
    "            #get all directory of this path\n",
    "            subpath=os.listdir(path)\n",
    "            #loop over these diroctories\n",
    "            for k, d in enumerate(subpath):\n",
    "                #append the label of image to array\n",
    "                labels.append(label)\n",
    "                #get the path of image\n",
    "                imgpath = os.path.join(path, d)\n",
    "                #read the image\n",
    "                imgs = io.imread(imgpath)\n",
    "                #get the DLTP feature of image then store it in features array\n",
    "                features.append(feature_DLTP(imgs))\n",
    "                #print this every 10 images\n",
    "                if k > 0 and k % 10== 0:\n",
    "                    print(\"[INFO] processed {}/{}\".format(k, len(subpath)))\n",
    "    return features, labels            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(img):\n",
    "    return ((img-min(img.flatten()))/(max(img.flatten())-min(img.flatten())))*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To enhance image according to illumination:\n",
    "    # o\tsubtracts each pixel from its mean pixel value\n",
    "    # o\tdivides the mean subtracted pixels by their standard deviation \n",
    "    # o\tadd regularization parameter to the standard deviation  λ\n",
    "    # o\tadd small constant error to avoid computation errors €\n",
    "\n",
    "def global_contrast_normalization(image1,reg,err):\n",
    "    image=np.array(image1)\n",
    "    mean=np.mean(image)\n",
    "    image=(image-mean)/(max(err,math.sqrt((reg)+np.mean((image-mean)**2))))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features,labels,precentage):\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=random_seed)\n",
    "    return train_features, test_features, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the string labels into codes  \n",
    "def encode(labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    return le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_image(im):\n",
    "    plt.imshow(im,cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset =r'CK+48'\n",
    "img_filenames = os.listdir(path_to_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'contempt', 'disgust', 'fear', 'happy', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "predictlist=img_filenames = os.listdir(path_to_dataset)\n",
    "print(img_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label        :         Count\n",
      "===================================\n",
      "anger                  135\n",
      "___________________________________\n",
      "contempt                  54\n",
      "___________________________________\n",
      "disgust                  177\n",
      "___________________________________\n",
      "fear                  75\n",
      "___________________________________\n",
      "happy                  207\n",
      "___________________________________\n",
      "sadness                  84\n",
      "___________________________________\n",
      "surprise                  249\n",
      "___________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Label        : \"+\"        Count\")\n",
    "print(\"===================================\")\n",
    "for i, fn in enumerate(img_filenames):\n",
    "        label = fn.split('.')[0]\n",
    "        path = os.path.join(path_to_dataset, fn)\n",
    "        subpath=os.listdir(path)\n",
    "        \n",
    "        i=len(subpath)\n",
    "        print(label+\"                 \",i)\n",
    "        print(\"___________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data and extract the features and labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset. This will take time ...\n",
      "[INFO] processed 10/135\n",
      "[INFO] processed 20/135\n",
      "[INFO] processed 30/135\n",
      "[INFO] processed 40/135\n",
      "[INFO] processed 50/135\n",
      "[INFO] processed 60/135\n",
      "[INFO] processed 70/135\n",
      "[INFO] processed 80/135\n",
      "[INFO] processed 90/135\n",
      "[INFO] processed 100/135\n",
      "[INFO] processed 110/135\n",
      "[INFO] processed 120/135\n",
      "[INFO] processed 130/135\n",
      "[INFO] processed 10/54\n",
      "[INFO] processed 20/54\n",
      "[INFO] processed 30/54\n",
      "[INFO] processed 40/54\n",
      "[INFO] processed 50/54\n",
      "[INFO] processed 10/177\n",
      "[INFO] processed 20/177\n",
      "[INFO] processed 30/177\n",
      "[INFO] processed 40/177\n",
      "[INFO] processed 50/177\n",
      "[INFO] processed 60/177\n",
      "[INFO] processed 70/177\n",
      "[INFO] processed 80/177\n",
      "[INFO] processed 90/177\n",
      "[INFO] processed 100/177\n",
      "[INFO] processed 110/177\n",
      "[INFO] processed 120/177\n",
      "[INFO] processed 130/177\n",
      "[INFO] processed 140/177\n",
      "[INFO] processed 150/177\n",
      "[INFO] processed 160/177\n",
      "[INFO] processed 170/177\n",
      "[INFO] processed 10/75\n",
      "[INFO] processed 20/75\n",
      "[INFO] processed 30/75\n",
      "[INFO] processed 40/75\n",
      "[INFO] processed 50/75\n",
      "[INFO] processed 60/75\n",
      "[INFO] processed 70/75\n",
      "[INFO] processed 10/207\n",
      "[INFO] processed 20/207\n",
      "[INFO] processed 30/207\n",
      "[INFO] processed 40/207\n",
      "[INFO] processed 50/207\n",
      "[INFO] processed 60/207\n",
      "[INFO] processed 70/207\n",
      "[INFO] processed 80/207\n",
      "[INFO] processed 90/207\n",
      "[INFO] processed 100/207\n",
      "[INFO] processed 110/207\n",
      "[INFO] processed 120/207\n",
      "[INFO] processed 130/207\n",
      "[INFO] processed 140/207\n",
      "[INFO] processed 150/207\n",
      "[INFO] processed 160/207\n",
      "[INFO] processed 170/207\n",
      "[INFO] processed 180/207\n",
      "[INFO] processed 190/207\n",
      "[INFO] processed 200/207\n",
      "[INFO] processed 10/84\n",
      "[INFO] processed 20/84\n",
      "[INFO] processed 30/84\n",
      "[INFO] processed 40/84\n",
      "[INFO] processed 50/84\n",
      "[INFO] processed 60/84\n",
      "[INFO] processed 70/84\n",
      "[INFO] processed 80/84\n",
      "[INFO] processed 10/249\n",
      "[INFO] processed 20/249\n",
      "[INFO] processed 30/249\n",
      "[INFO] processed 40/249\n",
      "[INFO] processed 50/249\n",
      "[INFO] processed 60/249\n",
      "[INFO] processed 70/249\n",
      "[INFO] processed 80/249\n",
      "[INFO] processed 90/249\n",
      "[INFO] processed 100/249\n",
      "[INFO] processed 110/249\n",
      "[INFO] processed 120/249\n",
      "[INFO] processed 130/249\n",
      "[INFO] processed 140/249\n",
      "[INFO] processed 150/249\n",
      "[INFO] processed 160/249\n",
      "[INFO] processed 170/249\n",
      "[INFO] processed 180/249\n",
      "[INFO] processed 190/249\n",
      "[INFO] processed 200/249\n",
      "[INFO] processed 210/249\n",
      "[INFO] processed 220/249\n",
      "[INFO] processed 230/249\n",
      "[INFO] processed 240/249\n",
      "Finished loading dataset.\n"
     ]
    }
   ],
   "source": [
    "print('Loading dataset. This will take time ...')\n",
    "features, labels = load_dataset()\n",
    "print('Finished loading dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels=split_data(np.array(features),np.array(labels),0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 8128)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(train_features).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step I want to reduce the number of input features. \n",
    "# As High-dimensional features affect the performance of classifiers,\n",
    "\n",
    "# features from DLTP have high-dimensional which contain a lot of redundant information.\n",
    "# Too many features slow down the classification process and lead to degradation in the accuracy of the classifier. \n",
    "# So, I use Principal component analysis (PCA) in this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.7)\n",
    "X_train1 = pca.fit_transform(train_features)\n",
    "X_test1 = pca.transform(test_features)\n",
    "train_features=X_train1\n",
    "test_features=X_test1\n",
    "train_labels=train_labels\n",
    "test_labels=test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 14)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(train_features).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-ELM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The K-ELM classifier is the kernalized variant of the extreme learning machine (ELM) classifier;\n",
    "# The classical ELM is a single-layer Feed-forward neural network classifier which requires a large number of hidden nodes,\n",
    "# those results in higher computational complexity and a longer training time, so I improve this classifier and \n",
    "# introduce a new variation of ELM classifier which is K-ELM classifier\n",
    "# •K-ELM classifier:\n",
    "# It uses kernels that map the features into higher dimensional space. Also, the RBF kernels required is \n",
    "# much less in K-ELM than the hidden nodes in a conventional ELM classifier.\n",
    "# •\tkernel technique:\n",
    "# It states that given two input vectors xi and xj, \n",
    "# the dot product of their mapped features represented by h(xi) . h(xj) can be replaced by a kernel function Ω(xi; xj).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(X, Y=None,g=None):\n",
    "    k=rbf_kernel(X, Y, gamma=g)\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input:\n",
    "# 1/sigma : control the width of gaussian kernel [gamma dec underfitting] [gamma inc overfitting]\n",
    "#c : regularization parameter ,It aids in improving the generalization performance of the classifier \n",
    "# x , y : two input vectors training and test\n",
    "#Output : Calculate the output weight of KELM network ß\n",
    "\n",
    "def KELM_Beta(sigma,c,x,y):\n",
    "    #convert to numrical values\n",
    "    le.fit(y)\n",
    "    y = le.transform(y)\n",
    "    #count the emotions\n",
    "    emotion = len(np.unique(y))\n",
    "    #calculate One_Hot_Encoding\n",
    "    One_Hot_Encoding=np.zeros((emotion,emotion), np.uint8)\n",
    "    #loop over the emotions\n",
    "    for em in range (emotion):\n",
    "        One_Hot_Encoding[em][em]=1\n",
    "    #map from numrical values to One_Hot_Encoding    \n",
    "    T = One_Hot_Encoding[y, :]    \n",
    "    N,d=x.shape\n",
    "    #Define the kernel matrix of K-ELM = h(xi) . h(xj)\n",
    "    Omega=kernel(x,None,sigma)\n",
    "    #get indintity matrix of lenght N\n",
    "    I=np.eye(N)\n",
    "    #Calculate the output weight of KELM network ß\n",
    "    Beta = np.linalg.inv((I /c) + Omega).dot(T)    \n",
    "    return Beta,One_Hot_Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input:\n",
    "# 1/sigma : control the width of gaussian kernel [gamma dec underfitting] [gamma inc overfitting]\n",
    "# x , x_test : two input vectors training and test\n",
    "#Output : output the class that has the maximum magnitude\n",
    "\n",
    "def KELM_Output(sigma,x,x_test,Beta,One_Hot_Encoding):\n",
    "    hx=kernel(x_test,x,sigma)\n",
    "    #output nodes in vectroized form\n",
    "    fx= hx.dot(Beta)\n",
    "    #the classifier selects the output node that has the maximum magnitude as the output class\n",
    "    p=np.argmax(fx,axis=1)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_pred,test_labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    yt = le.fit_transform(y_pred)    \n",
    "    rt = le.fit_transform(test_labels)\n",
    "    eqw = sum(map(eq, list(rt), list(y_pred)))\n",
    "    size=len(rt)\n",
    "    acc=(eqw/size)*100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 97.46192893401016\n",
      "train 100.0\n"
     ]
    }
   ],
   "source": [
    "Beta,One_Hot_Encoding=KELM_Beta(sigma,c,np.array(train_features),np.array(train_labels))\n",
    "p1=KELM_Output(sigma,np.array(train_features),np.array(test_features),Beta,One_Hot_Encoding)\n",
    "print(\"test\",acc(p1,test_labels))\n",
    "\n",
    "p2=KELM_Output(sigma,np.array(train_features),np.array(train_features),Beta,One_Hot_Encoding)\n",
    "print(\"train\",acc(p2,train_labels))\n",
    "\n",
    "# test 97.46192893401016\n",
    "# train 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9746192893401016\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98        20\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       1.00      0.90      0.95        41\n",
      "           3       1.00      1.00      1.00        15\n",
      "           4       0.93      1.00      0.96        39\n",
      "           5       0.96      1.00      0.98        22\n",
      "           6       1.00      0.98      0.99        54\n",
      "\n",
      "    accuracy                           0.97       197\n",
      "   macro avg       0.98      0.98      0.98       197\n",
      "weighted avg       0.98      0.97      0.97       197\n",
      "\n",
      "[[20  0  0  0  0  0  0]\n",
      " [ 0  6  0  0  0  0  0]\n",
      " [ 1  0 37  0  3  0  0]\n",
      " [ 0  0  0 15  0  0  0]\n",
      " [ 0  0  0  0 39  0  0]\n",
      " [ 0  0  0  0  0 22  0]\n",
      " [ 0  0  0  0  0  1 53]]\n"
     ]
    }
   ],
   "source": [
    "test_labels1=encode(test_labels)\n",
    "print(accuracy_score(test_labels1, p1))\n",
    "print(classification_report(test_labels1, p1))\n",
    "print(confusion_matrix(test_labels1, p1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred label: happy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "img = io.imread(r'CK+48\\happy\\S010_006_00000013.png')\n",
    "f=feature_DLTP(img)\n",
    "fp= pca.transform([f])\n",
    "\n",
    "p1=KELM_Output(sigma,np.array(train_features),np.array(fp),Beta,One_Hot_Encoding)\n",
    "\n",
    "print ('pred label:', predictlist[p1[0]] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "91bb753b057673435fb8d6f6a083e6c818364728098c7ae050ca3a25357dd754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
